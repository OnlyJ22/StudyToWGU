Here are the **main keywords and concepts** from the lesson on **Mainframe Computers**:

---

## Mainframe Computers

### **Definition & Purpose**

* **Mainframe**: Large, powerful computer system used to process **very large amounts of data** quickly.
* Designed for **multiple simultaneous users**.
* Typically used by **institutions** (banks, airlines, corporations).

---

## Development of Mainframes

* **1950s Origin** ‚Äì still used today.
* Optimized for **high input/output volumes**.
* Performance measured in **MIPS** (Million Instructions Per Second).
* Required **dedicated rooms**, cooling, and power systems.

---

## Capabilities

* Handles **hundreds of users** at once.
* Used for **transaction processing**, large-scale computation.
* Components: **Motherboard, CPU, Memory** (more powerful than PCs).
* Processes **massive data securely and reliably**.

---

## Present-Day Use

* Still used in:

  * **Banking**
  * **Airline Reservations**
  * **E-commerce**
  * **Inventory & Shipping**
* IBM: **Dominates market** (\~90% share).
* Modern mainframes: Powerful and expensive (can cost \$100,000+).

---

## Related Systems

### **Supercomputer**

* Designed for **specific, complex scientific tasks**.
* Not used for transaction processing.

### **Server**

* Provides services (e.g., databases, web hosting).
* Can be a **personal computer or a mainframe**.
* **‚ÄúServer‚Äù** = role, not hardware type.

---

## Summary Keywords

* **Mainframe**: High-volume data processing system
* **Components**: CPU, memory, motherboard
* **Transaction Processing**: Key use-case
* **Supercomputer**: Complex scientific calculations
* **Server**: Service-focused role (not hardware-specific)

Here are the **main keywords and concepts** from the lesson on **Parallel Computing**:

---

## Parallel Computing

### **Core Concept**

* **Parallel Computing**: Dividing a task into smaller parts and executing them **simultaneously** on multiple processors or computers.

---

### **Need for Speed**

* Society demands **faster results** in all areas (e.g., fast food, weather forecasts).
* Complex tasks like **weather prediction** require **multiple processors**.

---

### **Performance Characteristics**

* **Speedup**: Main benefit; multiple processors reduce execution time.
* **Limitations**:

  * Task may not be fully divisible.
  * Uneven division of workload.
  * **Overhead** from splitting and managing tasks.

---

### **Simple Example**

* Equation:
  `Y = (4√ó5) + (1√ó6) + (5√ó3)`

  * **Serial**: Step-by-step on one processor
  * **Parallel**: All multiplications done at once, then summed

---

### **Real-World Examples**

* **Weather forecasting** ‚Äì massive data sets processed quickly
* **Movie special effects** ‚Äì computed with **large parallel infrastructures**
* **Everyday applications** ‚Äì modern desktops use parallelism (e.g., MS Word, Outlook)

---

### **Key Terms**

* **Overhead** ‚Äì extra time or resources needed to manage parallel tasks
* **Divisibility** ‚Äì how easily a task can be split into parallel parts
* **Speedup** ‚Äì improvement in performance with additional processors

Here are the **main keywords and concepts** from the lesson on **Distributed and Parallel Computing**:

---

## Distributed and Parallel Computing

### **Computer Processing Power**

* Computing power has increased (faster CPUs, more memory, larger storage).
* Modern problems (e.g., drug simulation) require **more processing than a single computer** can handle.

---

### **What Is Distributed Computing?**

* **Definition**: Coordinated use of **multiple independent computers** over a network to achieve a **common goal**.
* **Uses**:

  * Break large tasks into **parallelizable subtasks**.
  * Assign subtasks to **multiple machines**.
  * Share data via **messages**.
  * Reduce total processing **time**.
* **Components**:

  * **Master computer**: Manages and coordinates tasks.
  * **Worker computers**: Execute subtasks and return results.
* **Example**: **SETI\@home** ‚Äì volunteer distributed computing with millions of idle computers.

---

### **Challenges in Distributed Computing**

* Task must be divisible.
* Requires controller software.
* Needs communication (message passing) between systems.
* Resource management and task scheduling are essential.

---

### **Other Applications**

* **Sensor networks**: Distributed data collection (e.g., environmental monitoring).
* **Weather forecasting**
* **Massively multiplayer online games (MMOGs)**: Shared processing across systems.

---

### **What Is Parallel Computing?**

* **Definition**: Using **multiple processors** in one computer that share **a single memory**.
* **Example**: Dual-core, quad-core processors.
* **Advantage**: Increases power of a single machine.
* **Limitation**: Scalability limited by shared memory and architecture.

---

### **Key Differences**

| Aspect        | Distributed Computing | Parallel Computing                  |
| ------------- | --------------------- | ----------------------------------- |
| Units         | Multiple computers    | Multiple processors in one computer |
| Memory        | Each has own memory   | Shared memory                       |
| Communication | Via network messages  | Via shared memory                   |
| Scalability   | Highly scalable       | Limited scalability                 |

---

Here are the **main keywords and concepts** from the lesson on **Types of Computer Processing**:

---

## Types of Computer Processing

### **Processor (CPU)**

* **Definition**: The "brain" of the computer that performs tasks, analyzes instructions, and produces results.
* **Key Role**: Determines how **fast** and **efficient** a computer is.

---

### **Multiprocessing**

* **Definition**: A system with **multiple processors or cores** working **simultaneously** to complete tasks.
* **Benefits**:

  * Perform **multiple tasks at once**.
  * Increased **speed** and **efficiency**.
  * Ideal for multitasking (e.g., running multiple applications).

---

### **Core**

* **Dual-core**: Two independent processing units.
* **Quad-core**: Four independent processing units.
* **Each core**:

  * Has its **own memory**.
  * Can run **separate threads**.
  * Appears as **multiple logical processors**.

---

### **Key Processor Components**

* **Clock Speed**:

  * Measured in **GHz (gigahertz)**.
  * Indicates how **fast** a processor can execute instructions.
  * Example: 2.42 GHz is faster than 1.30 GHz.
* **Cache**:

  * Measured in **MB (megabytes)**.
  * Temporary memory for storing frequently accessed data.
  * **Larger cache** = **better efficiency**.
  * Similar to a **scratch pad** for quick notes.

---

### **Use Cases for Multiprocessing**

* **Everyday tasks**: Running word processors, spreadsheets, accounting software simultaneously.
* **High-intensity tasks**:

  * **CT scans**
  * **Image processing**
  * **Data analysis**

---

### **Lesson Summary**

* **Multiprocessing** allows for simultaneous task execution.
* **More cores = faster performance**.
* **Processor performance** depends on both **clock speed** and **cache size**.
* **Modern computers** use dual, quad, or higher-core CPUs for faster, efficient performance.

---

Here‚Äôs a concise summary and key concepts from the **‚ÄúUsing Technology to Solve Problems: Grid Computing‚Äù** lesson:

---

## üß† **Using Technology to Solve Problems**

### üåê **Grid Computing**

**Definition**:
A type of **distributed computing** where **independent computers** (nodes) are networked together to perform large tasks **in parallel**, making them act like one powerful computer.

---

## üîß **How It Works**

* **Tasks** are **broken into smaller parts**.
* These parts are **distributed** across **multiple computers** (nodes).
* Each part is processed **simultaneously**.
* Final results are **combined** for a complete solution.

**Example**:
Instead of computing
`Z = (2√ó3)+(1√ó7)+(4√ó6)`
**sequentially**, Grid Computing lets each multiplication happen **at the same time**, saving time.

---

## üß© **Components of Grid Computing**

Similar to a regular desktop, but on a much larger scale:

* **Processors** ‚Äì Perform calculations (hundreds or thousands in a grid).
* **Memory** ‚Äì Temporary data storage.
* **Storage** ‚Äì Long-term data holding.
* **Software** ‚Äì Controls tasks, splits jobs, coordinates nodes.

**Key Differences from a Desktop:**

1. **Scale**: Massive resources (thousands of processors).
2. **Orchestration Software**: Manages task distribution and coordination.
3. **High-Speed Networking**: Connects all nodes for data sharing and communication.

---

## üè¢ **Real-World Applications**

* **Tech giants**: Facebook, LinkedIn, Twitter ‚Äì use grids for performance and scalability.
* **Cloud providers**:

  * **Amazon Web Services (AWS)**
  * **Google Cloud**
  * **Microsoft Azure**

These platforms offer **grid-like computing power on-demand**, reducing hardware costs for businesses.

---

## üìò **Lesson Summary**

* Grid Computing = **many networked computers** working together as one.
* Useful for solving **large, complex problems** faster.
* **Key benefits**:

  * Speed
  * Scalability
  * Cost efficiency (especially for enterprises).
* You likely use services powered by Grid Computing every day.

---

Here‚Äôs a breakdown of the **main keywords and concepts** from the lesson on **Flynn‚Äôs Taxonomy and Parallelism**:

---

## üß† **Parallelism & Flynn‚Äôs Taxonomy**

### üîÑ **Types of Parallelism**

* **Data Parallelism**:
  Multiple processors execute **the same task** on **different data subsets**.
  *Example: Each TA grades full exams separately.*

* **Task Parallelism**:
  Multiple processors execute **different tasks** on **the same or shared data**.
  *Example: Each TA grades different questions on all exams.*

---

## üß± **Flynn‚Äôs Taxonomy**

**Based on two streams**:

* **Instruction stream** (task)
* **Data stream**

### üîπ **SISD ‚Äì Single Instruction, Single Data**

* Traditional, sequential computing.
* One processor, one task, one data set.
* **Example**: Single-core CPU.

---

### üîπ **SIMD ‚Äì Single Instruction, Multiple Data**

* One instruction stream across **multiple processors**.
* Each processor operates on **different data**.
* **Highly efficient** and synchronized at instruction level.
* **Example**: Vector processors, GPUs.

---

### üîπ **MISD ‚Äì Multiple Instructions, Single Data**

* Rare architecture.
* Multiple algorithms applied to the **same data**.
* **Example**: Testing different filters on one image.

---

### üîπ **MIMD ‚Äì Multiple Instructions, Multiple Data**

* Most **common modern** architecture.
* Each processor runs a different instruction on its own data.
* **Example**: Multicore CPUs.

---

## üîÑ **Extensions to Flynn‚Äôs Taxonomy**

### üîπ **SPMD ‚Äì Single Program, Multiple Data**

* Same program runs on all processors.
* **Instruction execution can differ per core**.
* Synchronization is at the **program level** (not instruction level).
* **Widely used** in modern parallel programming.

---

### üîπ **MPMD ‚Äì Multiple Programs, Multiple Data**

* Each processor/core runs **different programs** on **different data**.
* Programs collaborate on a **shared problem**.
* **Example**: Hybrid cloud applications, distributed systems.

---

## üìå **Key Terms Recap**

* **Flynn‚Äôs Taxonomy**
* **Data Parallelism / Task Parallelism**
* **SISD, SIMD, MISD, MIMD**
* **SPMD, MPMD**
* **Synchronization**
* **Instruction Stream / Data Stream**

---

Here are the **main keywords and concepts** from the lesson on **Cache and Distributed Caching**:

---

## üß† **Cache & Distributed Cache**

### üîπ **Cache (Definition)**

* Reserved storage for **quicker data retrieval**
* Stores **frequently accessed** or **recently used** data
* Improves **response time** and **system performance**
* Exists as **software** or **hardware memory**

---

### üîπ **Example (Analogy)**

* Remembering the square root of 111,111 the second time = **cached memory**

---

### üîπ **Distributed Cache**

* Cache system **spread across multiple nodes/servers**
* Shares cached data **across network**
* Data stays cached while **frequently accessed**

#### **Main Features**

1. **Global resource access**
2. **Scalability** and **performance consistency**
3. **High availability** even if a node fails

---

## üèóÔ∏è **Distributed Cache Architecture Types**

### **Type 1: SMP (Symmetric Multiprocessing)**

* Multiple **identical processors**
* **Shared main memory**
* **Single OS**
* Boosted **throughput** for solving single problems

### **Type 2: DFM (Distributed File Management & Smart Disk System)**

* Uses **multiple local disks**
* Acts as **local/remote file caches**
* Enables **global and local access** to data

### **Type 3: WWW (World Wide Web Level)**

* **Client cache**: Improves data latency & accessibility
* **Server cache**: Distributes **database workload**
* **Network cache**: Shares data from previously accessed content

---

## üìå **Key Terms Recap**

* **Cache**
* **Distributed cache**
* **SMP architecture**
* **DFM system**
* **WWW caching layers**: client, server, network
* **Scalability**, **high availability**, **performance**

---

Here are the **main keywords and concepts** from the lesson on **Parallel Computing, Flynn's Taxonomy, Superscalar, and VLIW Architecture**:

---

## ‚öôÔ∏è **Parallel Computing & Instruction-Level Parallelism**

### üîπ **Parallel Computing**

* Task broken into **units**
* Units ‚Üí **sets of instructions**
* Executed **simultaneously** across multiple CPUs
* Requires **synchronization** and careful **coordination**

---

## üß© **Flynn‚Äôs Taxonomy (1966)**

Classifies systems based on instruction & data stream:

### **1. SISD (Single Instruction, Single Data)**

* Traditional **single-core** systems
* One instruction on one data segment

### **2. SIMD (Single Instruction, Multiple Data)**

* Same instruction on **multiple data sets**
* Common in graphics and scientific computing

### **3. MISD (Multiple Instruction, Single Data)**

* Multiple instructions on **one data segment**
* Rare, used in **testing/filtering** tasks

### **4. MIMD (Multiple Instruction, Multiple Data)**

* **Most general** architecture
* Different processors run **different instructions** on **different data**
* All modern **multicore CPUs**

---

## üöÄ **Superscalar Architecture**

### **Definition**

* Executes **multiple instructions** per clock cycle
* Uses **dynamic scheduling** (at runtime)
* Introduced in **Second-Generation RISC**

### **Key Components**

* **Instruction-level parallelism**
* **Pipeline stages** (like assembly line)
* **Dependency checking** at runtime
* **Throughput** increases with multiple instructions processed in parallel

### **Limitations**

* Limited by **instruction dependencies**
* **Pipeline stalls** due to data dependency
* Overhead in **dependency checking**

---

## üß† **VLIW (Very Long Instruction Word) Architecture**

### **Definition**

* Packs **multiple independent instructions** together
* Uses **static scheduling** (at compile time)
* Compiler handles:

  * **Instruction bundling**
  * **Dependency checking**
  * **Execution scheduling**

### **Key Features**

* **Simple hardware** (no dynamic scheduling logic)
* Low **power consumption**
* Efficient when instructions are **clearly independent**

### **Limitations**

* **Portability issues** (machine-dependent code)
* **Cache misses** may cause **execution delays**
* Hard to optimize for **complex branching**

---

## üìå **Key Terms Recap**

* **Parallel computing**
* **Flynn's taxonomy**: SISD, SIMD, MISD, MIMD
* **Instruction-level parallelism**
* **Superscalar architecture**
* **Pipeline stalls**
* **VLIW architecture**
* **Dynamic vs. static scheduling**
* **Compiler-based execution (VLIW)** vs **hardware-based (Superscalar)**

---

Here are the **main keywords and core concepts** from the lesson on **Vector Processing**:

---

## üßÆ **Vector Processing**

### üîπ **Definition**

* A technique for **high-intensity data processing** using **vectors** (dynamic arrays).
* Handles **large sets of variables** simultaneously as one entity.
* Speeds up computing by **processing multiple data elements in parallel**.

---

## ‚öôÔ∏è **Key Characteristics**

* **Efficient**: One operation affects all vector elements at once.
* **Dynamic sizing**: Vectors can grow/shrink at runtime.

---

## üíª **Vector Processor**

* A **CPU** designed for **parallel processing** using vectors.
* Ideal for **large data computations** like:

  * GIS (Geographic Information Systems)
  * Satellite imaging
  * Human genome mapping
  * AI
  * Seismic modeling
  * Weather prediction

---

## üß© **Components of Vector Processor**

1. **Vector Register File** ‚Äì Stores vector elements
2. **Vector Address Generator** ‚Äì Calculates start address and number of elements
3. **Vector Functional Units** ‚Äì Performs arithmetic/logical operations
4. **Vector Data Switch** ‚Äì Connects processor to memory modules

---

## üèóÔ∏è **Vector Processing Architectures**

### **1. Pipelined Vector Processors**

* **Sequential stages** (like an assembly line)
* Each stage processes part of the data
* Example: full name generation (first ‚Üí last ‚Üí full)

### **2. Parallel Array Processors**

* **Sub-processes** handled in **parallel arrays**
* Example: processing first and last names **simultaneously**

---

## ‚úçÔ∏è **Pseudocode Example**

### Without Vector:

* Declare multiple variables
* Repeated computation
* Longer runtime

### With Vector:

* Declare vector A, B
* Perform one operation across **entire array**
* Massive **time savings**

---

## üß† **Key Takeaways**

* **Vector = dynamic array** used for **parallel processing**
* **Vector processors** = CPUs built to handle these operations
* Saves time by **treating data arrays as single variables**
* Architectures: **Pipelined** & **Parallel Array Processing**

---

Here are the **main keywords and concepts** from the lesson on **Parallel Processing and Interconnection Networks**:

---

## **Parallel Processing**

### üîπ **Definition**

* Simultaneous execution of **computations on multiple processors**
* Used in high-performance tasks like **weather simulation**, **climate modeling**, etc.

---

## **Interconnection Networks**

### üîπ **Definition**

* Connects **Processing Elements (PEs)** to **Memory Elements (MEs)**
* Can include **Switching Elements (SEs)** for flexible routing
* Enables **high-speed data transfer** between nodes in a parallel computing system

---

## **Network Topology**

* The **layout/pattern** of how nodes (processors, memory, switches) are connected

---

## üîÑ **Types of Interconnection Networks**

### 1. **Static Networks**

* **Fixed connections** (hard-wired)
* No switching elements (i.e., **direct connections**)
* Not reconfigurable during execution

#### Static Topologies:

* **Pipeline**
* **Matrix**
* **Ring**
* **Torus**
* **Tree**
* **Star**
* **Hypercube**

#### Static Network Types:

* **Completely Connected Network (CCN)**: All nodes interconnected; fast but costly
* **Limited Connection Network (LCN)**: Some nodes linked; uses **efficient routing**

---

### 2. **Dynamic Networks**

* Use **switches** between nodes (i.e., **indirect connections**)
* **Reconfigurable**, even **during execution**
* **Scalable** for large systems

#### Examples:

* **Bus**
* **Crossbar**
* **Multistage Networks (MINs)**

---

## ‚úÖ **Key Takeaways**

* **Parallel processing** splits tasks among multiple processors for faster execution
* **Interconnection networks** are critical for data movement in parallel systems
* **Static networks**: Fixed wiring, predictable but rigid
* **Dynamic networks**: Flexible, switch-based, and reconfigurable
* **Topology** affects efficiency, scalability, and cost

---

Here are the **main keywords and concepts** from the lesson on **GPGPU (General Purpose Graphics Processing Unit)**:

---

## **GPGPU (General Purpose Graphics Processing Unit)**

### üîπ **Definition**

* A **GPU used for general computing** tasks beyond graphics (e.g., simulations, data processing)
* All modern GPUs are GPGPUs

---

### üîπ **Key Features**

* **Highly parallel processing**: Executes many tasks simultaneously
* **More cores than CPUs**: Allows better **multitasking and faster computation**
* **Lower clock speeds**, but **greater throughput** due to core count
* **Efficient for massive computations**: Ideal for AI, simulations, big data

---

## **Difference from Traditional GPU/CPU**

* GPU is **optimized for parallel tasks**, not just graphics
* GPGPU **uses GPU for non-graphical computation**
* Easier to **scale** (multiple GPUs > multiple CPUs)
* Supports **standard programming languages** (via Cuda, OpenCL)

---

## **CUDA (Compute Unified Device Architecture)**

### üîπ **Definition**

* A **parallel computing platform by Nvidia**
* Allows programming GPUs using **C/C++**
* Exposes **CUDA cores** to developers

### üîπ **Benefits**

* Simplifies GPU programming
* Leverages GPU's full computational power
* CUDA cores = individual processors in GPU
  ‚Üí More CUDA cores = better **parallel performance**

---

## **GPGPU Use Cases**

* Scientific computing
* Weather simulation
* Machine learning
* Cryptography
* Financial modeling
* Real-time data processing

---

## **Coding Example**

* CUDA ‚ÄúHello World‚Äù program

  * Adds values to characters in parallel on GPU
  * Demonstrates basic use of **threads** and **memory allocation**

---

## ‚úÖ **Key Takeaways**

* GPGPU = **modern GPU used for general purpose, high-speed parallel computing**
* CUDA = **toolkit for programming Nvidia GPUs**
* GPGPU performance relies on **massive parallelism** and **number of CUDA cores**
* Has shifted GPU usage from **graphics-only** to **general computing powerhouse**

---

Here's a **summary of the key concepts and keywords** from the lesson on **Distributed Parallel Computing**:

---

## üíª **Distributed Parallel Computing**

### üîπ **Definition**

A **computing model** that uses:

* **Parallel computing** (multiple processors running simultaneously)
* **Distributed computing** (computers connected via network across various locations)

‚Üí Combines **scalability** of distributed systems with **speed** of parallel computing.

---

## üîπ **Comparison: Distributed vs. Parallel Computing**

| **Distributed Computing**                           | **Parallel Computing**                                |
| --------------------------------------------------- | ----------------------------------------------------- |
| Network of computers                                | Multiple processors                                   |
| Shares resources                                    | Performs high-speed computations                      |
| Focuses on **availability** and **fault tolerance** | Focuses on **concurrency**                            |
| Tolerates individual failures                       | Requires synchronization                              |
| Examples: cloud storage, email, banking             | Examples: scientific simulations, matrix computations |

---

## üîπ **Benefits of Distributed Parallel Computing**

* **Resource Sharing** (CPU, memory, storage)
* **Scalability** (add more nodes easily)
* **High Availability** (fault-tolerant)
* **Increased Computational Power** (use many machines together)

---

## üîπ **Examples**

* **Internet**
* **Cloud & Grid Computing** (e.g., AWS, Google Cloud)
* **Peer-to-peer networks**
* **Email services**
* **Online banking & travel systems**
* **Distributed supercomputers**

---

## üõ∞Ô∏è **SETI Project (Search for Extraterrestrial Intelligence)**

* Scientific experiment using distributed parallel computing
* Launched by UC Berkeley in 1999
* Goal: Analyze radio telescope data to detect extraterrestrial life
* **Volunteer-based**: Anyone with a computer and internet can help
* Program runs as a **screensaver**, analyzes data when idle
* Success: Demonstrated viability of **volunteer-based distributed computing**

---

## ‚úÖ **Lesson Takeaways**

* **Distributed computing** = computers working together via network
* **Parallel computing** = simultaneous task execution across processors
* **Combined** = **distributed parallel computing** for massive, complex computations
* **SETI project** = real-world example of using millions of personal computers for scientific research

---

























