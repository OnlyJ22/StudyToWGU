### Section 1: What Is Open Addressing?

#### Understanding Hash Collisions

In a **hash table**, data is stored at a location determined by a **hash key**. The hash key is processed through a **hash function** to compute the **storage index**.

But what if **two keys hash to the same location**?

> This is called a **collision**, and **open addressing** is one strategy to handle it.

---

#### What Is Open Addressing?

**Open addressing** means all entries are stored **within the hash table itself**, and no linked structures (like chains) are used. When a collision occurs, the algorithm searches for the next available spot using a **probing method**.

Each table slot can be:

* **Empty** — nothing has ever been stored
* **Occupied** — a key-value pair is present
* **Deleted** — a value existed but was removed (marked for deletion)

When retrieving data, **deleted** slots are still considered **occupied** to preserve search accuracy.

---

#### Insertion Rules (Key `k`):

1. Compute the base position using the hash function.
2. If the spot is **empty or deleted**, insert `k`.
3. If the spot is **occupied**, find an alternative spot using a **probing method**.

---

#### Probing Techniques in Open Addressing

#### 1. Linear Probing

If a collision occurs, check the **next slot** in sequence.

**Formula:**
`newIndex = (currentIndex + 1) mod tableSize`

```java
H(k) = k mod 10

Insert 12 → 12 mod 10 = 2 → place at index 2  
Insert 22 → 22 mod 10 = 2 → collision!  
→ (2+1) mod 10 = 3 → place at index 3  
Insert 52 → 52 mod 10 = 2  
→ (2+1) mod 10 = 3 (occupied)  
→ (3+1) mod 10 = 4 → place at index 4
```

**Drawback:**
Creates **clustering** — many values end up near each other, causing performance issues.

---

#### 2. Quadratic Probing

Instead of checking the next slot, use the **square of the probe number**.

**Formula:**
`newIndex = (baseIndex + i²) mod tableSize`

```java
Insert 12 → H(12) = 2 → place at index 2  
Insert 22 → H(22) = 2  
→ i = 1: (2 + 1²) mod 10 = 3 → index 3 → insert  
Insert 52 → H(52) = 2  
→ i = 1: index 3 (occupied)  
→ i = 2: (2 + 4) mod 10 = 6 → insert at index 6
```

**Drawback:**
May only access **half** of the table slots in some cases → can cause **infinite loops**.

---

#### 3. Double Hashing

Uses a **second hash function** to compute the step size for probing.

**Formula:**
`newIndex = (baseIndex + i × h2(k)) mod tableSize`
Where `h2(k)` is a secondary hash function.

Example:
Let `h(k) = k mod 10`, `h2(k) = 1 + (k mod 7)`

```java
Insert 22 → H(22) = 2  
→ h2(22) = 1 + (22 mod 7) = 2  
→ (2 + 1×2) mod 10 = 4 → insert at index 4  

Insert 52 → H(52) = 2  
→ h2(52) = 1 + (52 mod 7) = 4  
→ (2 + 1×4) mod 10 = 6 → insert at index 6
```

**Advantage:**
Reduces clustering by varying step size using key-based computation.

**Drawback:**
Performance declines as the table becomes **more full**.


#### Quick Comparison of Probing Methods

| Method            | Formula                    | Behavior                          |
| ----------------- | -------------------------- | --------------------------------- |
| Linear Probing    | `(h(k) + i) mod m`         | Checks next slot sequentially     |
| Quadratic Probing | `(h(k) + i²) mod m`        | Jump distances grow quadratically |
| Double Hashing    | `(h(k) + i × h2(k)) mod m` | Step size varies with second hash |

---

#### Important Reminder About Indexing

* Arrays are **0-indexed**:

  * Index `0` is the first slot
  * Index `1` is the second slot, and so on
* When computing `mod tableSize`, always ensure the result **wraps around** if it exceeds the table size.

---

#### Lesson Summary

* **Open addressing** is a technique used to resolve **hash collisions** in a fixed-length table.
* It keeps all data **within the array**, using probing to find available slots.
* There are **three main probing strategies**:

  1. **Linear Probing** → simple, but causes clustering
  2. **Quadratic Probing** → better distribution, but limited reach
  3. **Double Hashing** → best spread, but slower as table fills
* All methods trade **simplicity, performance, and collision resistance**.
* A solid understanding of open addressing helps build efficient hash-based data structures.

### Section 2: Robin Hood Hashing — “Stealing” Keys

#### Revisiting Hash Tables and Collisions

A **hash table** is a data structure that stores data as **key–value pairs**, where a **hash function** converts a key into an **array index**. But when two keys hash to the same index, a **collision** occurs.

There are several techniques for resolving collisions:

* **Chaining** – each index holds a linked list of entries
* **Open Addressing** – the table searches for the next available slot

However, with open addressing, some values may be stored **far from their original index**, leading to **slower performance**.

---

#### The Problem with Distance

If too many values are pushed far from their ideal position due to collisions, lookups require **more probes**, increasing **time complexity**.

> In hashing, closer is better.
> The farther a value is from its hash index, the more **inefficient** the table becomes.

---

#### What Is Robin Hood Hashing?

**Robin Hood hashing** is an **open addressing strategy** that minimizes the **variance in probe sequence lengths** by balancing distances.

> It "steals" from keys that are **closer to their home** and gives to those that are **farther away**.

---

#### Robin Hood Hashing Rules

When inserting a new value:

1. **Compute the hash index** for the key.
2. If the slot is empty, insert the key.
3. If the slot is occupied, compare the **probe distance** of the existing value vs. the new value.
4. If the new key has **traveled farther**, **swap** them.
5. Repeat the process for the displaced key until it finds a position.

---

#### Example Behavior

Consider a hash table using open addressing:

```
Insert A → Index 4  
Insert B → Index 4 → collision!  
→ B travels to 5  
Insert C → Index 4 → collision!  
→ Compare C's distance vs. A's distance  
→ If C has traveled more, it replaces A (Robin Hood style)
```

Each key stores or tracks its **probe count** (how far it has moved from its original index). The key with a **greater probe count** gets priority.

---

#### Why “Robin Hood”?

* Keys closer to their home are considered **“rich”**
* Keys farther away are considered **“poor”**
* Poorer keys get to **steal slots** from richer ones

This **flattens probe sequences**, reducing **maximum search times**.

---

#### Benefits of Robin Hood Hashing

* **Lower variance** in lookup times
* **Faster failed searches** — you can end searches early
* **No linked lists or extra memory overhead**
* Achieves **O(1)** time complexity in many cases
* Typical **maximum probe count** is around **6**
* Fewer cache misses due to flat array structure

---

#### What If a Key Isn’t Found?

Robin Hood handles **unsuccessful searches** efficiently:

* You can **stop searching** once the current probe exceeds the expected maximum
* Fewer probes are needed compared to standard open addressing
  (≈6 vs. ≈70 probe count in worst-case)

---

#### Variations on Robin Hood

##### Organ-Pipe Search

* Searches where the **“poor” key** might have been pushed to
* Instead of starting at the hash index, it starts at the **expected displaced region**

##### Smart Search

* **Finds the mean probe distance**, then searches **outward**
* Covers fewer locations compared to linear scans
* Slight impact on **cache performance**, but often faster in large tables

---

#### Lesson Summary

* A **hash table** maps keys to indices using a hash function.
* **Collisions** occur when multiple keys map to the same index.
* **Robin Hood hashing** is a collision-resolution strategy where:

  * Items that are **farther from their ideal index** can **replace** closer items
  * This balances probe sequences and **reduces search times**
* **Key features** of Robin Hood hashing:

  * Consistently lower probe counts
  * Efficient even when no match is found
  * Avoids clustering better than linear probing
* **Variants** like **organ-pipe search** and **smart search** optimize the lookup process further.

Robin Hood hashing gives us a powerful alternative to traditional open addressing — one that’s both fair and fast.

### Section 3: Map Data Structure

#### What Is a Map?

A **map** is a data structure that stores **key–value pairs**.

* Each **key** is unique
* A **value** may be duplicated and shared across multiple keys

> Think of a map like an **index**:
> The **key** helps you quickly retrieve the associated **value**.

---

#### Example of a Map

| Key         | Value |
| ----------- | ----- |
| Denver      | MST   |
| Chicago     | CST   |
| Minneapolis | CST   |
| New York    | EST   |
| Los Angeles | PST   |
| Miami       | EST   |

Note that multiple cities share the same value (`CST`, `EST`), but each city (key) is **unique**.

---

#### Common Map Methods

Key operations on maps include:

* `put()` – insert key/value pairs
* `get()` – retrieve value by key
* `containsKey()` – check if a key exists
* `containsValue()` – check if a value exists
* `size()` – return the number of key/value pairs

---

#### Maps in Java

In Java, the `Map` interface is implemented by several classes:

* `HashMap`
* `TreeMap`
* `EnumMap`
* `LinkedHashMap`
* `IdentityHashMap`
* `WeakHashMap`

Each implementation has **different performance characteristics** and use cases.

---

#### Working With Java Maps

#### Creating and Inserting Elements

```java
Map<String, Integer> arrayCars = new HashMap<>();
arrayCars.put("Toyota", 6);
arrayCars.put("Ford", 10);
arrayCars.put("Honda", 5);
arrayCars.put("Chrysler", 4);
arrayCars.put("Honda", 10); // overwrites the previous "Honda" entry
```

---

#### Copy All Entries With `putAll()`

```java
Map<String, Integer> newArrayCars = new HashMap<>();
newArrayCars.putAll(arrayCars);
```

---

#### Iterating Over Keys

**Using an Iterator:**

```java
Iterator<String> iterator = arrayCars.keySet().iterator();
while (iterator.hasNext()) {
    String key = iterator.next();
    Integer value = arrayCars.get(key);
    System.out.println("Key: " + key + ", Value: " + value);
}
```

**Using For-Each:**

```java
for (Object key : arrayCars.keySet()) {
    Object value = arrayCars.get(key);
}
```

**Using Stream with Lambda:**

```java
arrayCars.keySet().stream().forEach((value) -> {
    System.out.println(value);
});
```

---

#### Iterating Over Entries

**Using Entry Iterator:**

```java
Iterator<Map.Entry<String, Integer>> iterator2 = arrayCars.entrySet().iterator();
while (iterator2.hasNext()) {
    Map.Entry<String, Integer> entry = iterator2.next();
    String key = entry.getKey();
    Integer value = entry.getValue();
}
```

**Using For-Each with Entry Set:**

```java
for (Map.Entry<String, Integer> entry : arrayCars.entrySet()) {
    String key = entry.getKey();
    Integer value = entry.getValue();
}
```

---

#### Removing Entries

**Remove a key:**

```java
arrayCars.remove("Honda");
```

**Clear all entries:**

```java
arrayCars.clear();
```

---

#### Other Java Map Implementations

#### TreeMap

A **TreeMap** sorts keys in **natural order**.

```java
TreeMap<String, Integer> cars = new TreeMap<>();
cars.put("Honda", 1);
cars.put("Ford", 3);
cars.putIfAbsent("Porsche", 18);
System.out.println("TreeMap of cars: " + cars);
```

**Output:**

```
TreeMap of cars: {Ford=3, Honda=1, Porsche=18}
```

---

#### EnumMap

An **EnumMap** uses an `enum` type as the key.

```java
enum Months { JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, DEC }

EnumMap<Months, String> months = new EnumMap<>(Months.class);
months.put(Months.JAN, "Skiing");
months.put(Months.FEB, "Date Night");
months.put(Months.JUL, "Water Ski");
months.put(Months.MAR, "Shovel more SNOW");
months.put(Months.DEC, "Snowball Fight");
months.put(Months.JUN, "Camping");

System.out.println("EnumMap: " + months);
```

**Output:**

```
EnumMap: {JAN=Skiing, FEB=Date Night, MAR=Shovel more SNOW, JUN=Camping, JUL=Water Ski, DEC=Snowball Fight}
```

---

#### LinkedHashMap

A **LinkedHashMap** preserves the **insertion order** of keys.

```java
LinkedHashMap<Integer, String> linkedhm = new LinkedHashMap<>();
linkedhm.put(500, "Honda");
linkedhm.put(600, "Accura");
linkedhm.put(200, "Mazda");
linkedhm.put(400, "Toyota");

for (Map.Entry<Integer, String> m : linkedhm.entrySet()) {
    System.out.println(m.getKey() + " " + m.getValue());
}
```

---

#### IdentityHashMap

An **IdentityHashMap** compares keys using **reference equality** (`==`) instead of `.equals()`.

```java
Map<String, String> idhashmap = new IdentityHashMap<>();
idhashmap.put("Sandra", "55901");
idhashmap.put("Paul", "22903");
idhashmap.put(new String("Dave"), "50587");
idhashmap.put("Edgar", "50000");
idhashmap.put("Luisha", "72500");

System.out.println("Size of IdentityHashMap is: " + idhashmap.size());
```

---

#### WeakHashMap

A **WeakHashMap** allows garbage collection to automatically remove keys when no longer used.

```java
WeakHashMap<String, Integer> numbers = new WeakHashMap<>();
String honda = new String("Honda");
Integer hondaValue = 2;
String porsche = new String("Porsche");
Integer porscheValue = 4;

numbers.put(honda, hondaValue);
numbers.put(porsche, porscheValue);
System.out.println("WeakHashMap: " + numbers);

honda = null;
System.gc();  // Triggers garbage collection

System.out.println("WeakHashMap after garbage collection: " + numbers);
```

**Output:**

```
WeakHashMap: {Honda=2, Porsche=4}
WeakHashMap after garbage collection: {Porsche=4}
```

---

#### Lesson Summary

* A **map** stores **key–value pairs**, where each key is unique.
* Java provides multiple **Map implementations**:

  * **HashMap** – fast but unsorted
  * **TreeMap** – sorted by keys
  * **EnumMap** – uses enums for keys
  * **LinkedHashMap** – maintains insertion order
  * **IdentityHashMap** – compares keys by reference
  * **WeakHashMap** – allows keys to be garbage collected
* **Functional programming features** like **lambda functions** and **streams** allow concise and expressive map iteration.
* Choosing the right map depends on your needs for **ordering**, **performance**, and **memory efficiency**.

### Section 4: Sorted Maps

#### What Is a Sorted Map?

A **sorted map** is an extension of the **map data structure** in which the **keys are stored in a sorted order**. That order can be:

* **Natural order** – alphabetical or numerical
* **Custom order** – defined using a **comparator**

In Java, the `TreeMap` class implements this behavior.

> No matter the insertion order, a TreeMap will always **sort the keys** automatically.

---

#### Real-World Analogy

Martin helped his sister sell cookies. They noted each **house number (key)** and the **type of cookie purchased (value)**. To distribute the cookies efficiently, they sorted everything by **house number** — just like how a **TreeMap** sorts its keys.

---

#### Automatic Ordering Using TreeMap

```java
TreeMap<Integer, Integer> student = new TreeMap<>();
student.put(9, 30);
student.put(10, 25);
student.put(5, 15);
student.put(12, 10);
student.put(8, 40);

for(Map.Entry m : student.entrySet()) {
    System.out.println("Key = " + m.getKey() + " Value = " + m.getValue());
}
```

Even though keys were added in random order, the **TreeMap automatically sorts** them in ascending order:

**Output:**

```
Key = 5 Value = 15  
Key = 8 Value = 40  
Key = 9 Value = 30  
Key = 10 Value = 25  
Key = 12 Value = 10
```

---

#### Finding the First and Last Entry

* **First Entry** — use `firstEntry()`

```java
System.out.println("First entry in the database is: " + student.firstEntry());
```

**Output:**

```
First entry in the database is: 5=15
```

* **Last Entry** — use `lastEntry()`

```java
System.out.println("Last entry in the database is: " + student.lastEntry());
```

**Output:**

```
Last entry in the database is: 12=10
```

---

#### Floor Key and Ceiling Key

**floorKey(k)**:
Returns the **largest key ≤ k**

```java
System.out.println("Greatest ID ≤ 6: " + student.floorKey(6));
```

**Output:**

```
Greatest ID ≤ 6: 5
```

**ceilingKey(k)**:
Returns the **smallest key ≥ k**

```java
System.out.println("Ceiling Key = " + student.ceilingKey(7));
```

**Output:**

```
Ceiling Key = 8
```

If the key exists:

```java
System.out.println("Try for 9 = " + student.ceilingKey(9));
```

**Output:**

```
Try for 9 = 9
```

---

#### Higher Key and Lower Key

**higherKey(k)**:
Returns the **smallest key strictly greater than k**

```java
System.out.println("Higher Key = " + student.higherKey(10));
```

**Output:**

```
Higher Key = 12
```

**lowerKey(k)**:
Returns the **largest key strictly less than k**

```java
System.out.println("Lower Key = " + student.lowerKey(5));
```

**Output:**

```
Lower Key = null
```

---

#### Extracting a Sub-Map

Use `subMap(fromKey, toKey)` to get a **portion** of the map:

```java
System.out.println("The students that volunteered in Grade 5 are: " + student.subMap(10, 20));
```

**Output:**

```
The students that volunteered in Grade 5 are: {10=25, 12=10}
```

Note:

* `fromKey` is **inclusive**
* `toKey` is **exclusive**

---

#### Lesson Summary

* A **TreeMap** is a type of map that **sorts its keys** automatically.
* The **insertion order does not matter** — keys are stored in **ascending order**.
* Use `firstEntry()` and `lastEntry()` to get the **smallest and largest keys**.
* Use `floorKey()` and `ceilingKey()` to find keys that are **≤ or ≥** a target key.
* Use `higherKey()` and `lowerKey()` to find **immediate neighbors**.
* Use `subMap(fromKey, toKey)` to **extract a range** of entries from the map.
* TreeMaps **do not allow null keys** and will throw an exception if you try to insert one.

TreeMap makes sorting and querying key-value data seamless — ideal when order matters in your application logic.

### Section 5: What Is Separate Chaining?

#### Introduction to Hash Tables and Collisions

A **hash table** is a data structure used to **map keys to values** using a **hash function**. It consists of two main parts:

1. The **table**, where values are stored
2. The **hash function**, which converts keys into index positions

> The hash function assigns each key a position in the table, but sometimes **two different keys map to the same index**. This is called a **collision**.

---

#### Why Do Collisions Happen?

* Memory is **finite**, so it’s not possible to have a unique index for every possible key
* Collisions are usually **evenly distributed**, but they are **inevitable**

To handle this, one common strategy is **separate chaining**.

---

#### What Is Separate Chaining?

**Separate chaining** is a **collision resolution** technique that associates a **linked list** with every index in the hash table.

> If multiple keys hash to the same index, they are stored in a **chain (linked list)** at that index instead of being rejected.

---

#### Example – Handling Collisions

Suppose we hash a list of names to a table of size 6:

```
Input: [John, Janet, Mary, Martha, Claire, Jacob, Philip]
```

Using a hash function, names are assigned indices:

* Janet and Martha both hash to the same index
* Instead of replacing one, both are stored in a **linked list** at that index

**Without chaining** → collision causes overwrite or probing
**With chaining** → index contains a chain: `Janet → Martha`

---

#### Key Concept

With **separate chaining**:

* Each table cell holds a **linked list** (not just a single item)
* New elements that hash to the same index are **appended to the list**
* The **hash function governs placement**, just like before

---

#### Advantages of Separate Chaining

1. **Simple to Implement**

   * Just add items to the appropriate list
   * No probing or shifting required

2. **No Capacity Limit**

   * You’re not limited by table size
   * Each index can hold **multiple elements**

3. **Robust Collision Handling**

   * Not as affected by poor hash functions
   * Avoids primary clustering

4. **Scales with Data**

   * Efficient even when number of keys is **unknown or very large**

---

#### Disadvantages of Separate Chaining

1. **Poor Cache Performance**

   * Linked list elements are **not stored contiguously**
   * Slower memory access compared to arrays

2. **Wasted Space**

   * Some table cells may remain **empty**
   * Space is used for **pointers and linked list nodes**

3. **Long Chains**

   * Multiple collisions at one index lead to **long chains**
   * Increases **search and retrieval time**

4. **Higher Storage Demand**

   * More memory is required to maintain the **linked lists**

---

#### Lesson Summary

* **Separate chaining** is a method to handle **hash collisions** using **linked lists** at each index.
* When multiple keys hash to the same location, they are stored in a **chain** instead of replacing existing values.
* This avoids overwrites and open addressing, and it provides **flexibility** and **scalability**.
* However, it comes at the cost of **slower search times**, **redundant storage**, and **poor cache performance**.
* Despite its drawbacks, separate chaining remains a **widely used technique** in hash table design, especially when the **data size is large** or the **collision rate is high**.
